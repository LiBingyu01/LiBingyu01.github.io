<p align="center">
 <img src="https://komarev.com/ghpvc/?username=LiBingyu01&label=Profile%20Views&color=0e75b6&style=flat" alt="è®¿é—®é‡ç»Ÿè®¡" />
</p>

<div align="center">
  <img src="https://raw.githubusercontent.com/LiBingyu01/LiBingyu01.github.io/main/me.png" width="130" height="130" style="border-radius: 50%;" alt="Bingyu Li Avatar"/>
  
  <h1>Hi there, I'm Bingyu Li (æç‚³ç…œ) ğŸ‘‹</h1>
  
  <h3>ğŸ¤– Ph.D. Student @ <a href="https://en.ustc.edu.cn/">USTC</a></h3>

  <p>
    <b>Advisors: Prof. <a href="https://scholar.google.com/citations?user=AHu3xw0AAAAJ">Xuelong Li</a></b>
  </p>

  <p>
   æˆ‘ä¸»è¦å…³æ³¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹åŠå…¶åœ¨è‡ªç„¶ã€é¥æ„Ÿå’Œæ°´ä¸‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚
   <br>
   <i>I focus on Multimodal Large Language Models (MLLMs) and Open-Vocabulary Vision.</i>
  </p>

  <p>
    <img src="https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white" alt="PyTorch">
    <img src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54" alt="Python">
    <img src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black" alt="Linux">
    <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker">
  </p>
</div>

---

### ğŸ”¬ Research Interests

My research interests lie in the intersection of **Computer Vision** and **Natural Language Processing**:

- ğŸ”­ **Multimodal LLMs**: Vision-Language Models, Video Understanding
- ğŸŒŠ **Open-Vocabulary Vision**: Segmentation for Natural, Remote Sensing, and Underwater Scenes
- ğŸ§  **Deep Learning**: Dataset Construction, Algorithm Design

---

### ğŸ”¥ News
- **[2025.11]** ğŸ‰ One paper (**RSKT-Seg**) is accepted by **AAAI 2026** as <font color="red">**Oral**</font> Presentation!
- **[2025.10]** ğŸ… Awarded the **National Graduate Scholarship (åšå£«ç”Ÿå›½å®¶å¥–å­¦é‡‘)**.
- **[2025.04]** ğŸ‰ One paper (**Stitchfusion**) is accepted by **ACM MM 2025** as <font color="red">**Oral**</font> Presentation!
- **[2025.04]** One paper is accepted to **Pattern Recognition (PR) 2025**.
- **[2024.09]** Started my Ph.D. journey at **University of Science and Technology of China (USTC)**.

---

### ğŸ“ Selected Publications

> Full list available on my [Google Scholar](https://scholar.google.com/citations?user=6AdA-O4AAAAJ&hl=zh-CN).

**Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing** <br>
**Bingyu Li**, H Dong, D Zhang, Z Zhao, J Gao, X Li <br>
*AAAI Conference on Artificial Intelligence (**AAAI**), 2026* <span style="color:red">**(Oral)**</span> <br>
[![Paper](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=flat-square&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2509.12040)
[![Code](https://img.shields.io/badge/Github-Code-black?style=flat-square&logo=github)](https://github.com/LiBingyu01/RSKT-Seg)

**Stitchfusion: Weaving any visual modalities to enhance multimodal semantic segmentation** <br>
**Bingyu Li**, D Zhang, Z Zhao, J Gao, X Li <br>
*ACM International Conference on Multimedia (**ACM MM**), 2025* <span style="color:red">**(Oral)**</span> <br>
[![Paper](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=flat-square&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2408.01343)
[![Code](https://img.shields.io/badge/Github-Code-black?style=flat-square&logo=github)](https://github.com/LiBingyu01/StitchFusion)

**MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment** <br>
**Bingyu Li**, F Wang, D Zhang, Z Zhao, J Gao, X Li <br>
*arXiv Preprint, 2025* <br>
[![Paper](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=flat-square&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.15398)
[![Code](https://img.shields.io/badge/Github-Code-black?style=flat-square&logo=github)](https://github.com/LiBingyu01/MARIS)

**Exploring the Underwater World Segmentation without Extra Training** <br>
**Bingyu Li**, T Huo, D Zhang, Z Zhao, J Gao, X Li <br>
*arXiv Preprint, 2025* <br>
[![Paper](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=flat-square&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2511.07923)
[![Code](https://img.shields.io/badge/Github-Code-black?style=flat-square&logo=github)](https://github.com/LiBingyu01/Earth2Ocean)

---

### âš¡ Recent Activity & Stats

<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=LiBingyu01&show_icons=true&theme=radical&hide_border=true&count_private=true" alt="Bingyu's GitHub Stats" />
  <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=LiBingyu01&layout=compact&theme=radical&hide_border=true" alt="Most Used Languages" />
</p>

---

### ğŸ”— Service & Connect

**Academic Service (Reviewer)**
- **Journals:** TGRS, PR
- **Conferences:** CVPR, ICLR, NeurIPS

**Contact Me**
- ğŸ“§ Email: `libingyu0205@mail.ustc.edu.cn`
- ğŸ  Homepage: [LiBingyu01.github.io](https://libingyu01.github.io/)
- ğŸ“ Google Scholar: [Bingyu Li](https://scholar.google.com/citations?user=6AdA-O4AAAAJ&hl=zh-CN)
